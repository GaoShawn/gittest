　　
基于混合内存的多核共享最底层Cache管理

摘要
　　近年来，在数据中心系统设计中，采用基于DRAM和NVM的混合内存成为减少系统功耗和提升系统运行效率的有效方法。考虑到NVM和DRAM之间，以及NVM的读写之间访问代价的不一致性，很多研究者提出了一些新型的Cache管理策略。但是这些策略都是对单核系统的考量，没有考虑多核环境对于混合内存访问的影响。
　　本文在考虑NVM和DRAM非一致性访问的基础上，增加了对不同核访问特性的考量，提出了一种对共享最后一级Cache的动态插入和提升策略。运行spec2006的结果显示，与原有的方法相比，该策略在性能上平均提升**%，相对于原有的Cache大小仅需要0.1%的额外空间开销。

一 背景
　　在当前的计算机系统，尤其是数据中心系统的设计中，功耗和可扩展性成为一种重要的考虑因素。而内存的功耗占据了系统总体功耗的40%-50%【1】。由于DRAM自身的特性（静态刷新功耗和有限的可扩展性），使其无法满足当前系统设计的需求。最近出现的新型非易失性存储器（NVM）【2】被认为是比较有前景的传统DRAM替代品。与DRAM相比，NVM有更低的功耗，可扩展大，且与DRAM有相近的读访问速度。不过NVM也有三个显著的缺点：写延迟大，写功耗高，存在写次数限制。为了最大化利用不同存储介质的优势，在降低功耗的同时不严重影响效率，一些研究者们提出了采用DRAM和NVM混合内存的方式。
　　当前的混合内存管理架构一般有两种形式(fig 1)，（i）左图中NVM和DRAM位于同等层次，采用统一的地址空间（ii）右图所示，由少量的DRAM作为PCM的Cache。两种架构各有优缺点，在本文中我们只关注前者。
　　
　　　　　　　　　　　　　　　　Fig1：混合内存架构图
　　
　　为了更好的发挥DRAM访问速度快和NVM功耗低的优势，现有的关于混合内存的研究大都集中于DRAM和NVM之间页迁移策略的设计，从而使得频繁访问的页存储于DRAM，而访问较少的页放在PCM中【页迁移论文】。
　　考虑到页迁移不仅涉及到页的拷贝，同时也涉及Cache的更新，迁移过程会带来巨大的额外开销。传统的Cache替换策略（LRU）是为了统一内存而设计的，并没有考虑到混合内存访问的非对称性。因此LRU策略并不能得到一种最优的效果[4]。论文【HAP】首次提出了基于Cache划分的混合内存管理策略（HAP）。由于读操作为阻塞式访问，因此HAP采用读延迟作为访问NVM和DRAM的代价。虽然PCM的写操作不在程序的主路径上，不过由于PCM写具有较高的延迟和功耗，因此论文[WBA]提出了一种根据不同内存类型和内存操作设置Cache行优先级的策略。然而，这些新型的基于混合内存的Cache管理策略都没有考虑多核系统对于混合内存访问的影响。
　　在本文中，我们提出了一种多核环境下共享最后一级Cache的管理策略。在多核系统中，不同的程序，或者运行在多核上相同程序的不同线程可能具有不同的内存访问特性。同时在混合内存系统中，不同核对于不同类型内存的访问也存在差异。在我们的设计中，同时对这两个方面进行了考量。首先，需要一个额外的硬件去监控不同核的Cache利用率，以及对NVM和DRAM的访问情况。然后，根据所统计的信息确定不同核，以及相同核不同访问所对应的Cache行的插入和提升策略。在gem5模拟上运行的spec2006的结果显示，我们的策略相较与LRU和HAP都与明显的优势。本文的贡献如下：
　　1, 我们在考虑不同内存访问非对称性的同时，考虑了混合内存系统中多核的影响。通过实验说明了不同核中运行程序的Cache利用率，以及对于不同内存的访问存在差异；
　　2, 我们提出的最后一级Cache管理策略考虑了不同核，不同内存类型，以及不同操作的影响，提升了Cache的利用率，进一步达到了提升系统性能的目的；
　　3, 我们通过gem5模拟器和NVMain，运行spec2006对该算法进行评估，结果显示该算法平均获得***的性能的提升，最高提升***。同时系统功耗降低了***。
　　
　　
二 Motivation
1 多核Cache划分
　　　在不同核上同时执行的不同的应用程序或者相同应用程序的不同进程对于Cache的利用率是不同。考虑到Cache资源的有限性，我们需要考虑如何对不同核进行Cache资源的有效划分，从而使其得到最高效的利用。分配给每个核的Cache大小可以在软件层上进行，也可以在运行时通过一些硬件的监测来确定。我们可以在set的层次上对Cache进行划分，以使得不同核对应到不同的Cache set，这里涉及到页着色技术。而另一种基于way的Cache划分策略实现起来更加的简单，同时也能取得较好的效果。
　　　一种比较常用的基于way的Cache分区策略是通过一个额外的硬件设备来监测每个核的Cache利用情况，得到分配给每个核不同数目的Cache行时所获得的性能的提升[PIPP]。图2显示了两个核时的一个简单统计结果，当分别分配给core0核core1 5路和3路的时候系统能得到最高的命中次数。
　　　
　　　图2,分配给每个核不同数目的Cache行时的命中次数曲线
　　　但是在基于混合内存的系统中，由于NVM核DRAM读写代价的非对称性，最高的命中数目并不意味着最优的性能。为了解决这个问题，不能单纯的以命中次数作为性能优化的衡量标准，我们还需要考虑不同内存（NVM，DRAM）访问，以及不同内存操作（read，write）的影响。
　　　
2 NVM和DRAM的Cache划分
　　　原有的多核Cache管理都是针对于统一内存的，在统一DRAM的情况下对于每个核内部的数据不需要进一步的划分。但是在我们基于混合内存的系统中，确定完Cache划分后我们还需要进一步区分不同类型的内存。HAP认为在原有单核环境下混合内存Cache管理策略中读访问为阻塞式的操作，因此使用NVM和DRAM的读延迟作为衡量不同内存的标准。为了进一步缓解PCM写造成的巨大延迟和功耗，WBA力图使得NVM所对应的Cache行在Cache中停留更长的时间，从而最大限度的减少NVM的写回操作。与原有的策略不同，我们对于NVM和DRAM的划分是在多核Cache划分基础上进行的，因此具有更小的可操作性和灵活性。
　　　基于上述的考虑，首先我们需要在多核上Cache划分的时候考虑到不同内存之间，以及相同内存的不同操作之间的非对称性。另外在不同内存之间进行Cache划分的时候我们需要在有限的可操作空间中进行。本文的组织结构如下，第三部分描述了我们的多核Cache和不同内存Cache的划分策略；第四部分介绍了我们的实验方法以及得到的实验结果；第五部分对算法及其中的参数设置进行了简单的分析，最后对本文做了总结。

三 算法概述
　　　在这一部分我们首先简单描述了PIPP的Cache管理策略，然后分析了多核环境下混合内存的访问特性，并以此为依据，详细介绍了我们的共享最后一级Cache的替换策略。
1 PIPP的Cache管理策略
　　　对于n个核的系统，为每个核设置一个与原Cache具有相同结构的一组影子tag去记录如果当前核拥有所有的Cache时的命中信息。以LRU为例，在第i个最近访问位置（MRU）的命中代表该核占有i路时能保证此次命中，而占有i-1路的时候不能引发此次命中，并以此得出命中曲线。在得到每个核的命中曲线之后就可以计算出分别分配给每个核几路的时候能到达命中率的最优情况。
　　　假设得到的分配方式为I={a1,a2,a3……an}的时候能达到全局最优。在插入的时候，corei将新行插入到优先级为ai的位置，也就是说一个核的Cache划分结果决定了新Cache行的插入位置。而当Cache命中的时候，PIPP将按照一定的概率Pprom是命中的Cache行提升一位，而该行不发生变化的概率为1-Pprom。至于如何选择一个Cache行进行替换，PIPP采用和LRU相同的策略，选择优先级最低的Cache。
　　　如上文所述，在基于混合内存的系统中，由于不同内存访问的非一致性，所有我们不能单纯采用命中次数作为衡量标准。
　　　
2 多核Cache划分
　　由于内存读操作为阻塞式的访问，在HAP中采用NVM和DRAM的读延迟作为衡量内存访问代价的标准。由于读写队列的存在，在观察到NVM的写也可能会给系统的性能带来较大的影响之后，WBA增加了对NVM写的考量。DRAM和NVM的读写延迟比例大概为1:1:4.4:12[5]。
　　　在PIPP中，当影子tag发生命中的时候，此位置的计数器加1，而在我们的设计中，当发生一个命中的时候，该位置的计数器会加上相应的权值。例如发生一次DRAM读命中的时候，计数器加1，而发生一个NVM读命中的时候，计数器加4.4。另外由于对脏的Cache行发生一次命中，就相当于减少了一次写回操作。因此还需要在影子tag中添加一位用于记录该Cache行是否为脏。例如，当命中的NVM Cache行为脏时，相应计数器加12，而命中的DRAM Cache行为脏时，相应位置的计数器加1。
　　　通过设置权值的方式我们可以实现对NVM和DRAM的区分。同时，添加一个脏位用于记录当前行的状态。比较发现，对于DRAM而言，由于读写代价基本一致，所以命中行是否为脏没有影响。而对于NVM而言，由于NVM的row buffer命中率相当低【6】，所以在此处我们忽略了NVM row buffer命中的影响，直接认为NVM的脏行被替换的时候会导致一次写回操作。

3 不同内存Cache的划分
　　通过提出的Cache划分策略，我们确定了每个核应得到Cache行的数目，根据上文PIPP描述，也就确定了每个核新插入的Cache行的位置。但是，在混合内存中，由于NVM和DRAM访问的非一致性，我们必须进一步对NVM和DRAM进行划分。
　　　在PIPP中，当Cache命中的时候，对每个核而言，新数据的插入位置取决于该核在Cache划分得到的Cache行的数目，Cache划分中得到的Cache行越多，插入的位置也就越高。为此在进行NVM和DRAM划分的时候我们也通过调整Cache行的插入位置来实现。由于NVM有较高的读写延迟，因此我们应该适当提升NVM的优先级，同时降低DRAM的优先级。
　　　假设通过上文的Cache划分得到的Cache分配方式为I={a1,a2,a3……an}。当核i有新的Cache插入的时候，如果是DRAM，则插入到ai - DRAMoffset处，而如果是NVM则插入到（公式）ai +NVMOffset处。其中NVMOffset = DRAMOffset×NVMHit/（NVMHit+DRAMHit）*ratio。其中NVMHit和DRAMHit分别代表上一个取样周期中该核NVM Cache行和DRAM Cache行命中的次数，ratio代表NVM访问代价和DRAM访问代价的系数。当该取样周期中NVM命中数目较多时，说明该核对NVM访问比较频繁且有较好的局部性，因此该核的NVM行应该获得较高的优先级。为了过滤掉stream类型benchmark的少量命中对参数的影响，此处首先过滤掉Stream类型的核（命中率低于12.5%），即只对非stream核进行调整。
　　　同理，当Cache行发生命中的时候，也应该体现出NVM和DRAM的区别。在我们的设计中我们为核i命中的DRAM行提升ai /DRAMDivisor，而为命中的NVM行提升ai /NVMDivisor。其中NVMDivisor=DRAMDivisor/ratio。通过这种实现，当该核得到Cache划分越大，说明该核的局部性越好，其命中的Cache行可以获得更高的优先级的提升，同时也能体现出NVM和DRAM访问代价的不一致性。
　　　
4 实例

四 实验
　　　在这一部分，我们介绍了试验环境，评估方式和运行Spec2006部分benchmark的实验结果。
实验环境
　　　我们在gem5模拟器[9]和NVMain【10】组成的平台上进行模拟实现，这是受业界认可能精确模拟DRAM和NVM的实现方式。表1描述了处理器和主存的详细设置参数。每一级Cache行的大小都是64B，我们使用NVMain构建了两个内存模块，128M的DRAM和1GB的NVM。此处的NVM模块是基于PCM原型的，每一个内存模块具有一个请求队列，最大可以支持64个请求。表2描述了NVM和DRAM的功耗和访问延迟参数。其中array access和buffer access分别代表了实际操作内存单元和操作row buffer的功耗。
表1:系统配置参数
Processor	　　　4-cores, Out-of-Order, 2GHz
　　　L1 caches(per core)	　　　64KB I-cache, 64KB D-cache,
　　　2-cycle latency, 4-way associative
　　　L2 caches(per core)	　　　256KB, 8-way associative,
　　　10-cycle latency, write-back
　　　LLC(shared)	　　　8MB, 16-way associative,
　　　30-cycle latency,write-back
　　　DRAM	　　　128MB, open-page, FR-FCFS,
　　　row-buffer size=1KB
　　　NVM	　　　1GB, open-page, FR-FCFS,
　　　row-buffer size=64B[6]

表2: NVM和DRAM的时间和功耗参数
	NVM	DRAM


Energy(pJ/bit)	Array read	2.47	1.17
	Array write	16.82	0.39
	Buffer read	0.93	0.93
	Buffer write	1.02	1.02


Latency(ns)	tRCD	55	12.5
	tCL	12.5	12.5
	tWTR	7.5	7.5
	tRP	150	12.5

　　　根据[7]和[8],初始化内存分配，我们需要将一些只读或者更新非常不频繁的数据放在NVM中，其中包括一些代码段数据段等。考虑到DRAM空间本身就比较小，将堆区和栈区随机分配到DRAM或者NVM中。
　　　
测试集
　　　我们从SPEC CPU2006中选择了一些benchmarks作为我们评估的程序集。表3展示了我们用到的所有的benchmark。
表3: 测试集
workload	Description
	
	
	
	
	
	
	
	

　　对于每一个测试集，我们首先100 million条指令，用于把cache填满。然后对于测试集中的每一个benchmark运行125million条指令。当一个benchmark到达自身的指令数限制的时候，它继续执行去竞争Cache资源，但是我们仅仅统计指令125million条指令的结果[11,12]。因为每个benchmark指令的执行速度不一致，其中有些benchmark可能有更快的执行速度，因此在所有的benchmark全部执行完毕的时候，有些benchmark不只执行了125 million条指令。
　　对于所有的实验，我们根据一种SMT加速评估策略（公式）IPC[i]/IPCsa[i]，其中IPCsa为当前benchmark独享所有共享cache资源时候的IPC，IPC总体吞吐量为（公式）IPC[i]。而该加权加速的调和平均数（公式）c/(IPCsa[i]/IPC[i])代表了多核之间的公平性和性能。
　　　
实验结果
　　　每个核心抽取了32组，取平均，Ratio=3 DRAMOffset=2 DRAMDivisor=2
　　　

五 分析
参数设置分析
　　　Ratio
　　　DRAMOffset
　　　DRAMDivisor
Overhead分析
　　　表4展示了我们算法在储存上的额外开销。对于2M的L3cache，总共有2048个Cache set。在我们的算法中抽取了32个set作为样本，每100Million条指令进行一次更新，将每次更新的结果用到下一个100Million条指令中。每个影子Tag为15位，我们初步设置counter的大小为16位，经过测试在100Million条指令中，该值没有出现过越界的情况。另外每个Set设置两个变量由于记录NVM和DRAM各自的命中次数，每个变量设置了16，在测试中也未曾出现越界的情况。这样，总共需要的额外空间大小为131072Byte，占总Cache大小不到1%.
source	Overhead	Total Overhead
Tag	15*assoc*numberSet	61440Byte
counter	16*assoc*numberSet	65536Byte
Dirty Bit	1*assoc*numberSet	4096Byte
NVM,DRAM Hit counter	32*numberSet	65536Byte
功耗分析

六 总结
　　
　　
　　
　　
　　
　　
　　
　　
　　
　　
　　
[1] C. Lefurgy, K. Rajamani, F. Rawson, W. Felter, M. Kistler, and T. W. Keller. Energy management for commercialservers. IEEE Computer, 36(12):39C48, Dec. 2003.
[2]Burr G W, Breitwisch M J, Franceschini M, et al. Phase change memory technology[J]. Journal of Vacuum Science & Technology B, 2010, 28(2): 223-262.
[3]Dhiman G, Ayoub R, Rosing T. PDRAM: a hybrid PRAM and DRAM main memory system[C]//Design Automation Conferen ce, 2009. DAC'09. 46th ACM/IEEE. IEEE, 2009: 664-669.
[4]Zhang X, Hu Q,Wang D, Li C,Wang H. A read-write aware replacement policy for phase change memory. In Advanced Parallel Processing Technologies, Teman O, Yew P C, Zang B (eds), Springer, 2011, pp.31C45.
[5]B. C. Lee, E. Ipek, O. Mutlu, and D. Burger. Architecting phase change memory as a scalable dram alternative. ACM SIGARCH Computer Architecture News, 37(3):2C13, 2009.
[6]Meza J, Li J, Mutlu O. A case for small row buffers in non-volatile main memories[C]//Computer Design (ICCD), 2012 IEEE 30th International Conference on. IEEE, 2012: 484-485.
[7]S. Lee, H. Bahn, and S. H. Noh. Clock-dwf: A write-history-aware page replacement algorithm for hybrid pcm and dram memory architectures. IEEE Transactions on Computers, 63(9):2187C2200,2014.
[8]Y. Park, S. K. Park, and K. H. Park. Linux kernel support to exploit phase change memory. In Linux Symposium, volume 2010, pages 217C224. Citeseer, 2010.
[9]N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu, J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, et al. The gem5 simulator. ACM SIGARCH Computer Architecture News, 39(2):1C7, 2011.
[10]M. Poremba and Y. Xie. NVMain: An architectural-level main memory simulator for emerging non-volatile memories. In IEEE Computer Society Annual Symposium on VLSI (ISVLSI), pages 392C397. IEEE, 2012.
[11]A. Jaleel, W. Hasenplaugh, M. Qureshi, J. Sebot, S. S. Jr.,and J. Emer. Adaptive Insertion Policies for Managing Shared Caches. In Proc. of the 17th Int. Conference on Parallel Architectures and Compilation Techniques, 2007. 
[12]M. K. Qureshi and Y. N. Patt. Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches. In Proc. of the 39th Int. Symp. on Microarchitecture, pages 423C432, Orlando,FL, Dec. 2006.